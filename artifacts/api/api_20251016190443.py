"""
AutoAI Generated Prediction API

This API was automatically generated by the AutoAI AgentHub.
Model: Ridge
Task: regression

Author: AutoAI AgentHub
Version: 1.0
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import pandas as pd
import numpy as np
from typing import List, Dict, Any
import os

# Initialize FastAPI app
app = FastAPI(
    title="AutoAI Prediction API",
    description="Auto-generated prediction API",
    version="1.0.0"
)

# Load model and preprocessor
model_path = "artifacts\models\best_model_20251016190443.joblib"
model = joblib.load(model_path)

# Load preprocessor if available
preprocessor_path = model_path.replace("best_model_", "preprocessor_")
if os.path.exists(preprocessor_path):
    preprocessor = joblib.load(preprocessor_path)
else:
    preprocessor = None

# Feature names
FEATURE_NAMES = ['feature_0', 'feature_3', 'feature_1_A', 'feature_1_B', 'feature_1_C', 'feature_2_0', 'feature_2_1', 'feature_4_A', 'feature_4_B', 'feature_4_C']

# Pydantic models
class PredictionRequest(BaseModel):
    features: List[float]
    
    class Config:
        schema_extra = {
            "example": {
                "features": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
            }
        }

class PredictionResponse(BaseModel):
    prediction: Any
    confidence: Optional[float] = None
    model_info: Dict[str, Any]

class HealthResponse(BaseModel):
    status: str
    model_type: str
    task_type: str
    metrics: Dict[str, float]

@app.get("/", response_model=Dict[str, str])
async def root():
    """Root endpoint with API information."""
    return {
        "message": "AutoAI Prediction API",
        "model": "Ridge",
        "task": "regression",
        "version": "1.0.0"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="healthy",
        model_type="Ridge",
        task_type="regression",
        metrics={
    "mse": 1.1533,
    "rmse": 1.0739,
    "mae": 0.8194,
    "r2_score": -0.1016,
}
    )

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Make predictions using the trained model."""
    try:
        # Validate input length
        if len(request.features) != len(FEATURE_NAMES):
            raise HTTPException(
                status_code=400,
                detail=f"Expected {len(FEATURE_NAMES)} features, got {len(request.features)}"
            )
        
        # Create DataFrame
        df = pd.DataFrame([request.features], columns=FEATURE_NAMES)
        
        # Apply preprocessing if available
        if preprocessor is not None:
            df = preprocessor.transform(df)
        
        # Make prediction
        prediction = model.predict(df)[0]
        
        # Get prediction confidence/probability if available
        confidence = None
        if hasattr(model, 'predict_proba') and task_type == "classification":
            proba = model.predict_proba(df)[0]
            confidence = float(max(proba))
        
        return PredictionResponse(
            prediction=float(prediction) if isinstance(prediction, (int, float, np.number)) else str(prediction),
            confidence=confidence,
            model_info={
                "model_type": "Ridge",
                "task_type": "regression",
                "feature_count": len(FEATURE_NAMES)
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/info", response_model=Dict[str, Any])
async def model_info():
    """Get model information."""
    return {
        "model_type": "Ridge",
        "task_type": "regression",
        "target_column": "target",
        "feature_names": FEATURE_NAMES,
        "metrics": {
    "mse": 1.1533,
    "rmse": 1.0739,
    "mae": 0.8194,
    "r2_score": -0.1016,
},
        "timestamp": "2025-10-16T19:04:43.752686"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
