import os
import subprocess
import logging
import pandas as pd
import joblib
from datetime import datetime
from typing import Optional, Dict, Any
from src.core.dataclasses import ModelArtifact, DeploymentInfo


class DeployAgent:
    """Agent responsible for model deployment and interface generation."""
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        """Initialize the Deployment Agent.
        
        Args:
            config: Configuration dictionary
            logger: Logger instance
        """
        self.config = config
        self.logger = logger
        self.streamlit_dir = os.path.join(config.get("artifact_dir", "artifacts"), "streamlit")
        self.streamlit_port = config.get("streamlit_port", 8501)
        
        # Create directories
        os.makedirs(self.streamlit_dir, exist_ok=True)
        
        self.logger.info("DeployAgent: Initialized deployment agent")
    
    def generate_streamlit_script(self, model_artifact: ModelArtifact, output_path: Optional[str] = None) -> str:
        """Generate a Streamlit application script.
        
        Args:
            model_artifact: ModelArtifact containing model and metadata
            output_path: Optional custom output path
            
        Returns:
            Path to generated script
        """
        self.logger.info("DeployAgent: Generating Streamlit application script")
        
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
            output_path = os.path.join(self.streamlit_dir, f"streamlit_app_{timestamp}.py")
        
        # Extract metadata
        meta = model_artifact.meta if hasattr(model_artifact, 'meta') else {}
        feature_names = model_artifact.feature_names
        target_column = model_artifact.target_column
        task_type = model_artifact.task_type
        
        # Generate Streamlit script
        script_content = self._create_streamlit_template(
            model_artifact, feature_names, target_column, task_type
        )
        
        # Write script to file
        with open(output_path, "w", encoding='utf-8') as f:
            f.write(script_content)
        
        self.logger.info(f"DeployAgent: Streamlit script generated at {output_path}")
        return output_path
    
    def _create_streamlit_template(self, model_artifact: ModelArtifact, 
                                 feature_names: list, target_column: str, 
                                 task_type: str) -> str:
        """Create Streamlit application template."""
        
        # Format metrics for display
        metrics_str = "{\n"
        for metric, value in model_artifact.metrics.items():
            metrics_str += f'    "{metric}": {value:.4f},\n'
        metrics_str += "}"
        
        template = f'''"""
AutoAI Generated Streamlit Application

This application was automatically generated by the AutoAI AgentHub.
Model: {model_artifact.model_type}
Task: {task_type}
Generated: {model_artifact.timestamp}

Author: AutoAI AgentHub
Version: 1.0
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
from datetime import datetime

# Page configuration
st.set_page_config(
    page_title="AutoAI Model Demo",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {{
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }}
    .metric-card {{
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 0.5rem 0;
    }}
    .success-card {{
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }}
</style>
""", unsafe_allow_html=True)

# Header
st.markdown('<h1 class="main-header">ü§ñ AutoAI Model Demo</h1>', unsafe_allow_html=True)
st.markdown("---")

# Load model and preprocessor
try:
    model = joblib.load("{model_artifact.model_path}")
    preprocessor = joblib.load("{model_artifact.preprocessor_path}")
    st.sidebar.success("‚úÖ Model and Preprocessor loaded successfully!")
except Exception as e:
    st.sidebar.error(f"‚ùå Error loading model or preprocessor: {{e}}")
    st.stop()

# Sidebar information
st.sidebar.header("üìä Model Information")
st.sidebar.markdown(f"**Model Type:** {model_artifact.model_type}")
st.sidebar.markdown(f"**Task Type:** {task_type.capitalize()}")
st.sidebar.markdown(f"**Target Column:** {target_column}")
st.sidebar.markdown(f"**Generated:** {model_artifact.timestamp}")

# Display metrics
st.sidebar.header("üìà Performance Metrics")
metrics = {metrics_str}
for metric, value in metrics.items():
    st.sidebar.metric(
        metric.replace('_', ' ').title(),
        f"{{value:.4f}}"
    )

st.sidebar.markdown("---")

# Main content
st.header("üéØ Make Predictions")
st.write("Enter the feature values below to get a prediction from the trained model.")

# Create input form
input_data = {{}}
col1, col2, col3 = st.columns(3)

# Generate input widgets based on feature names
for i, feature in enumerate(feature_names):
    current_col = [col1, col2, col3][i % 3]
    with current_col:
        # Determine input type based on feature name patterns
        if any(keyword in feature.lower() for keyword in ['age', 'year', 'count', 'number']):
            input_data[feature] = st.number_input(
                f"{{feature.replace('_', ' ').title()}}",
                min_value=0.0,
                max_value=1000.0,
                value=0.0,
                step=1.0,
                key=f"input_{{feature}}"
            )
        elif any(keyword in feature.lower() for keyword in ['rate', 'score', 'ratio', 'probability']):
            input_data[feature] = st.number_input(
                f"{{feature.replace('_', ' ').title()}}",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.01,
                key=f"input_{{feature}}"
            )
        else:
            input_data[feature] = st.number_input(
                f"{{feature.replace('_', ' ').title()}}",
                value=0.0,
                step=0.01,
                key=f"input_{{feature}}"
            )

# Prediction button
st.markdown("---")
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    if st.button("üîÆ Make Prediction", type="primary", use_container_width=True):
        try:
            # Create DataFrame from input data
            input_df = pd.DataFrame([input_data])
            
            # Ensure all features are present and in correct order
            for feature in feature_names:
                if feature not in input_df.columns:
                    input_df[feature] = 0.0
            
            # Reorder columns to match training data
            input_df = input_df[feature_names]
            
            # Transform input data using preprocessor
            # Note: This assumes the preprocessor expects the original feature structure
            # For a robust solution, we would need to store the original feature mapping
            
            # Create a dummy DataFrame with original features for preprocessing
            # This is a simplified approach - in production, you'd want to store feature mapping
            original_features = {feature_names}  # Use feature names as original features
            
            # Create DataFrame with original feature structure
            processed_input_df = pd.DataFrame(columns=original_features)
            processed_input_df = pd.concat([processed_input_df, input_df], ignore_index=True).fillna(0)
            
            # Transform and predict
            transformed_input = preprocessor.transform(processed_input_df)
            prediction = model.predict(transformed_input)
            
            # Display results
            st.markdown("---")
            
            if task_type == "classification":
                prediction_text = f"Predicted Class: **{{prediction[0]}}**"
                st.markdown(f'<div class="success-card"><h3>üéØ {{prediction_text}}</h3></div>', unsafe_allow_html=True)
                
                # Show prediction probabilities if available
                if hasattr(model, 'predict_proba'):
                    probabilities = model.predict_proba(transformed_input)
                    st.subheader("üìä Prediction Probabilities")
                    
                    # Create probability chart
                    prob_df = pd.DataFrame(probabilities, columns=model.classes_)
                    st.bar_chart(prob_df.T)
                    
                    # Show probability table
                    st.subheader("üìã Detailed Probabilities")
                    for i, class_name in enumerate(model.classes_):
                        st.metric(f"Class {{class_name}}", f"{{probabilities[0][i]:.4f}}")
            else:
                prediction_text = f"Predicted Value: **{{prediction[0]:.4f}}**"
                st.markdown(f'<div class="success-card"><h3>üéØ {{prediction_text}}</h3></div>', unsafe_allow_html=True)
            
            # Show input summary
            with st.expander("üìã Input Summary"):
                st.json(input_data)
                
        except Exception as e:
            st.error(f"‚ùå Error making prediction: {{str(e)}}")
            st.exception(e)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>ü§ñ Generated by AutoAI AgentHub | Version 1.0</p>
    <p>This application demonstrates automated ML model deployment</p>
</div>
""", unsafe_allow_html=True)

# Additional information
with st.expander("‚ÑπÔ∏è About This Model"):
    st.markdown(f"""
    **Model Details:**
    - **Algorithm:** {model_artifact.model_type}
    - **Task Type:** {task_type}
    - **Target Variable:** {target_column}
    - **Features:** {len(feature_names)} features
    - **Training Date:** {model_artifact.timestamp}
    
    **Performance Metrics:**
    """)
    for metric, value in metrics.items():
        st.write(f"- **{{metric.replace('_', ' ').title()}}:** {{value:.4f}}")
    
    st.markdown("""
    **How to Use:**
    1. Adjust the feature values in the input form above
    2. Click "Make Prediction" to get a prediction
    3. Review the results and probability distributions
    4. Experiment with different input combinations
    """)
'''
        
        return template
    
    def launch_streamlit(self, script_path: str, port: Optional[int] = None) -> DeploymentInfo:
        """Launch the Streamlit application.
        
        Args:
            script_path: Path to Streamlit script
            port: Optional port number
            
        Returns:
            DeploymentInfo containing deployment information
        """
        self.logger.info(f"DeployAgent: Launching Streamlit app from {script_path}")
        
        if port is None:
            port = self.streamlit_port
        
        try:
            # Launch Streamlit in background
            process = subprocess.Popen([
                "streamlit", "run", script_path, 
                "--server.port", str(port),
                "--server.headless", "true"
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            app_url = f"http://localhost:{port}"
            
            self.logger.info(f"DeployAgent: Streamlit app launched on {app_url}")
            
            # Create deployment info
            deployment_info = DeploymentInfo(
                app_path=script_path,
                app_url=app_url,
                model_artifact=model_artifact,
                deployment_timestamp=datetime.now().isoformat(),
                status="running",
                port=port
            )
            
            return deployment_info
            
        except FileNotFoundError:
            self.logger.error("DeployAgent: Streamlit command not found. Please install Streamlit.")
            raise RuntimeError("Streamlit not found. Please install with: pip install streamlit")
        except Exception as e:
            self.logger.error(f"DeployAgent: Failed to launch Streamlit app: {str(e)}")
            raise RuntimeError(f"Failed to launch Streamlit app: {str(e)}")
    
    def create_prediction_api(self, model_artifact: ModelArtifact, output_path: Optional[str] = None) -> str:
        """Create a simple API endpoint for predictions.
        
        Args:
            model_artifact: ModelArtifact containing model and metadata
            output_path: Optional custom output path
            
        Returns:
            Path to generated API script
        """
        self.logger.info("DeployAgent: Creating prediction API")
        
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
            output_path = os.path.join(self.streamlit_dir, f"api_{timestamp}.py")
        
        # Generate API script
        api_content = self._create_api_template(model_artifact)
        
        # Write API script
        with open(output_path, "w", encoding='utf-8') as f:
            f.write(api_content)
        
        self.logger.info(f"DeployAgent: API script generated at {output_path}")
        return output_path
    
    def _create_api_template(self, model_artifact: ModelArtifact) -> str:
        """Create API template."""
        
        template = f'''"""
AutoAI Generated Prediction API

This API was automatically generated by the AutoAI AgentHub.
Model: {model_artifact.model_type}
Task: {model_artifact.task_type}

Author: AutoAI AgentHub
Version: 1.0
"""

from flask import Flask, request, jsonify
import pandas as pd
import joblib
import numpy as np
from datetime import datetime

app = Flask(__name__)

# Load model and preprocessor
try:
    model = joblib.load("{model_artifact.model_path}")
    preprocessor = joblib.load("{model_artifact.preprocessor_path}")
    print("‚úÖ Model and preprocessor loaded successfully")
except Exception as e:
    print(f"‚ùå Error loading model: {{e}}")
    exit(1)

@app.route('/')
def home():
    return jsonify({{
        "message": "AutoAI Prediction API",
        "model_type": "{model_artifact.model_type}",
        "task_type": "{model_artifact.task_type}",
        "features": {model_artifact.feature_names},
        "timestamp": "{model_artifact.timestamp}"
    }})

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get input data
        data = request.get_json()
        
        if not data:
            return jsonify({{"error": "No input data provided"}}), 400
        
        # Create DataFrame from input
        input_df = pd.DataFrame([data])
        
        # Ensure all features are present
        for feature in {model_artifact.feature_names}:
            if feature not in input_df.columns:
                input_df[feature] = 0.0
        
        # Reorder columns
        input_df = input_df[{model_artifact.feature_names}]
        
        # Transform and predict
        transformed_input = preprocessor.transform(input_df)
        prediction = model.predict(transformed_input)
        
        # Prepare response
        response = {{
            "prediction": prediction[0],
            "timestamp": datetime.now().isoformat()
        }}
        
        # Add probabilities for classification
        if "{model_artifact.task_type}" == "classification" and hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(transformed_input)
            response["probabilities"] = dict(zip(model.classes_, probabilities[0]))
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({{"error": str(e)}}), 500

@app.route('/health')
def health():
    return jsonify({{"status": "healthy", "timestamp": datetime.now().isoformat()}})

if __name__ == '__main__':
    print("üöÄ Starting AutoAI Prediction API...")
    print(f"üìä Model: {model_artifact.model_type}")
    print(f"üéØ Task: {model_artifact.task_type}")
    print("üåê API will be available at: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=True)
'''
        
        return template
